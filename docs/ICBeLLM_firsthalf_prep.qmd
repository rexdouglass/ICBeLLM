

```{r}
verbose=F
set.seed(1) 
Sys.setenv(RETICULATE_PYTHON = "/home/skynet3/miniconda3/bin/python3")
library(reticulate)
use_python("/home/skynet3/miniconda3/bin/python3")
crisno=196
```

# Shorten

```{python}

crisno=196

def shorten_prompt(prompt_func, story, sentence, new_tokens, *args): #pass in the function
  token_limit=2500 #this was supposed to be 2750 but I'm getting ooms at 2600??
  #
  story=story.split('References:')[0].strip() #first just try to shorten the story by removing referneces
  current_count = generator.tokenizer.encode(prompt_func(story,sentence, *args), return_mask = False).shape[1] + new_tokens
  if current_count > token_limit:
    tokens_over=current_count-token_limit
    final=prompt_func( generator.tokenizer.decode( generator.tokenizer.encode(story)[:,:-tokens_over] )[0] + " ...",sentence, *args) #Ok now we pull exactly the right amount of tokens off
    final_count = generator.tokenizer.encode(final, return_mask = False).shape[1]
    if final_count>token_limit:
      raise Exception("Guessed wrong and too many tokens")
    #print(generator.tokenizer.encode(final, return_mask = False).shape[1], flush=True)
    return(final)
  else:
    return(prompt_func(story,sentence, *args))



```


```{python}

import pandas as pd
import re


#https://github.com/turboderp/exllama/blob/master/example_basic.py
#!pip install flash-attn --no-build-isolation
import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:64"

import os
os.getcwd()
import sys
sys.path.insert(0, "/home/skynet3/Downloads/exllama/")
from model import ExLlama, ExLlamaCache, ExLlamaConfig

from tokenizer import ExLlamaTokenizer
from generator import ExLlamaGenerator
import os, glob
import torch

def icbe_llm_generator():

  #You are a natural language processing pipeline. You extract entities from text. Parse the text carefully and return every single person, place, and thing mentioned in the text.
  
  #I'm processing prompts at about 41 tokens a second and producing responses at about 14 tokens a second
  #/home/skynet3/Downloads/exllama
  #python test_benchmark_inference.py -d <path_to_model_files> -p -ppl
  #python example_chatbot.py -d "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_temp/Wizard-Vicuna-30B-Uncensored-GPTQ/" -un "Jeff" -p prompt_chatbort.txt
  #python webui/app.py -d "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_temp/Wizard-Vicuna-30B-Uncensored-GPTQ/"
  
  #python webui/app.py -d "/home/skynet3/Downloads/LLAMA/StableBeluga2-GPTQ/" -gs 17.2,24
  #python webui/app.py -d "/home/skynet3/Downloads/LLAMA/airoboros-l2-70B-gpt4-1.4.1-GPTQ/" -gs 17.2,24 -length 4096
  
  #model_directory =  "/home/skynet3/Downloads/LLAMA/airoboros-l2-70B-gpt4-1.4.1-GPTQ/"
  model_directory =  "/home/skynet3/Downloads/LLAMA/Platypus2-70B-Instruct-GPTQ/" #https://huggingface.co/garage-bAInd/Platypus2-70B-instruct
  #model_directory =  "/home/skynet3/Downloads/LLAMA/platypus-main/Platypus2-70B-Instruct-GPTQ/"
  #model_directory =  "/home/skynet3/Downloads/LLAMA/StableBeluga2-GPTQ-4ibt-32g/"


  # Directory containing model, tokenizer, generator
  
  #model_directory =  "/mnt/str/models/llama-13b-4bit-128g/"
  #model_directory =  "/home/skynet3/Downloads/LLAMA/Llama-2-13B-chat-GPTQ/"
  #model_directory =  "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_temp/Wizard-Vicuna-30B-Uncensored-GPTQ/"
  #model_directory =  "/home/skynet3/Downloads/LLAMA/LLaMA-30b-GPTQ/"
  #model_directory =  "/home/skynet3/Downloads/LLAMA/guanaco-33B-GPTQ/"
  #model_directory =  "/home/skynet3/Downloads/LLAMA/falcon-40b-instruct-3bit-GPTQ/"
  
  # Locate files we need within that directory
  
  tokenizer_path = os.path.join(model_directory, "tokenizer.model")
  model_config_path = os.path.join(model_directory, "config.json")
  st_pattern = os.path.join(model_directory, "*.safetensors")
  model_path = glob.glob(st_pattern)[0]
  
  # Create config, model, tokenizer and generator
  
  config = ExLlamaConfig(model_config_path)               # create config from config.json
  config.model_path = model_path                          # supply path to model weights file
  
  #Rex's special additions
  config.set_auto_map("17.2,24") #This did make it allocate to both #https://huggingface.co/TheBloke/guanaco-65B-GPTQ/discussions/21
  #config.set_auto_map("16.2,24") #"15.5. 24 is what I use.""  https://github.com/turboderp/exllama/issues/191
  config.max_input_len = 2900 #4096 #3072 #4096 #4096  #oh interesting if I raise this it actually lowers my tokens
  config.max_seq_len   = 2900 #3072 #4096 #I don't understand the difference between these two.
  config.flash_attn = 2900 #3072 #4096 #experimenting to see if this works #this one is wire to input length.
  
  model = ExLlama(config)                                 # create ExLlama instance and load the weights
  tokenizer = ExLlamaTokenizer(tokenizer_path)            # create tokenizer from tokenizer model file
  
  tokenizer.encode('#') #396
  tokenizer.eos_token
  tokenizer.eos_token_id
  tokenizer.encode(tokenizer.eos_token)
  
  cache = ExLlamaCache(model)                             # create cache for inference
  
  # monkey patch generator simple to have a custom stop token
  def generate_simple_rex(self, prompt, max_new_tokens = 128, custom_stop=None):
      self.end_beam_search()
      ids, mask = self.tokenizer.encode(prompt, return_mask = True)
      self.gen_begin(ids, mask = mask)
      max_new_tokens = min(max_new_tokens, self.model.config.max_seq_len - ids.shape[1])
      eos = torch.zeros((ids.shape[0],), dtype = torch.bool)
      for i in range(max_new_tokens):
        token =  generator.gen_single_token(mask = mask)
        token_as_string =  generator.tokenizer.decode( token )[0]
        #print(token_as_string)
        if custom_stop in token_as_string:
          #print("breaking!")
          generator.sequence=generator.sequence[0,:-1] #strip off that last token
          break
        for j in range(token.shape[0]):
          if token[j, 0].item() ==  generator.tokenizer.eos_token_id: eos[j] = True
        if eos.all(): break
      text = self.tokenizer.decode(self.sequence[0] if self.sequence.shape[0] == 1 else self.sequence)
      return text
    
  ExLlamaGenerator.generate_simple_rex = generate_simple_rex #monkey patch in our change
  generator = ExLlamaGenerator(model, tokenizer, cache)   # create generator
  
  #generator.end_beam_search()
  #ids, mask = generator.tokenizer.encode("USER: Print 5 pounds signs, e.g. '#####' ASSISTANT:", return_mask = True)
  #generator.gen_begin(ids, mask)
  #token_as_string=tokenizer.decode(generator.gen_single_token(mask = mask))
  #'#####' #is a token. Hilarious.
  
  tokenizer.eos_token_id
  tokenizer.encode('a')
  tokenizer.decode(torch.tensor([[2]]))
  # Configure generator
  
  generator.disallow_tokens([tokenizer.eos_token_id])
  
  #Here is my attempt to make it as deterministic as possible.
  #0 temperature throws an error. top_k=1 supposedly ignores everything else and is perfectly deterministic.
  generator.settings.token_repetition_penalty_max = 1.0 #ok if you lower this to 0 it just repeats over and over again
  #big picture if you threshold with top_k =1 you can let tthe temp up a bit and p down and get the same result at much much faster perf.
  generator.settings.temperature = 0.2 #0.01 #0.95
  generator.settings.top_p = 0.8 #0.99 #https://github.com/turboderp/exllama/issues/81
  generator.settings.top_k = 1 #1 #https://github.com/turboderp/exllama/issues/81
  generator.settings.typical = 1.0 #https://github.com/turboderp/exllama/issues/81
  
  return(generator)




# Produce a simple generation

#prompt = "Once upon a time,"
#print (prompt, end = "")
#output = generator.generate_simple(prompt, max_new_tokens = 100)
#print(output[len(prompt):])
#[output]
#generator.generate_simple_rex("USER: Print 5 pounds signs, e.g. '#####' ASSISTANT:", max_new_tokens = 10, custom_stop="#" ) #

```

# Initialize (only do once or OOM)

```{python}

import numpy as np
generator=icbe_llm_generator()

#452 seconds on average for 3090+4080.
benchmark=False
from datetime import datetime
times=[]
if benchmark:
  for i in range(5):
    #generator.gen_begin('') #resets the cache don't have it working apparently #https://github.com/turboderp/exllama/discussions/155
    start_time = datetime.now()
    output_benchmark =  generator.generate_simple("### User: List the first 1000 things that come to mind.\n### ASSISTANT:", max_new_tokens = 4000  ) 
    end_time = datetime.now()
    times.append(end_time-start_time)

#pip install pyread
import pyreadr
crisis_narratives = pyreadr.read_r("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.Rds").popitem()[1]
print(crisis_narratives.keys())

#Load the story once for the whole thing
story=crisis_narratives.text[crisno-1] #remember 0 indexing

```

## Verify OOMm side

Looks like our limit is actually close to 2450. That's interesting. It's because I changed the max tokens window. It actually gave me less memory.
Ok 2800 gets me 2801
2900 gets me 2850 but not 2875

```{r}

library(tidyverse)
prompt=rep("\n",2850) %>% paste0(collapse='') #ok so \n is a single unicode character plus a stop character. Lots of others were weird. 
n_tokens=py$generator$tokenizer$encode(prompt)$shape[1]
print(n_tokens)
output =  py$generator$generate_simple_rex( prompt , max_new_tokens = as.integer(1) , custom_stop= '\n'  ) #

max_tokens=2800

```


# Begin Weak Supervision

## Train/Valid/Test Splits

```{r}

library(tidyverse)
library(stringi)
library(glue)

#New plan we're going to filter to only expert codings.
#Dead sentences have no actors listed in the wide, just use that.
events_agreed_long <- readRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/ICBe_V1.1_events_agreed_long.Rds")
events_agreed_long_filtered_experts_atleast2 <-  events_agreed_long %>% filter(sentence_span_text %>% str_detect("\\.$"))  %>% filter(expert==1) %>% #Removes 626 degenerate sentence 
                                                 group_by(crisno, sentence_number_int_aligned, sentence_span_text, #event_number_int
                                                 varname_normalized, value_normalized) %>%
                                                  filter(n()>=2) %>% #The idea here is that if 2 or more experts listed a country as actor_a then we're going to count it. 
                                                   #even if they were talking about different events
                                                 ungroup()



temp <- events_agreed_long_filtered_experts_atleast2 %>%
               mutate(do_actor_a= ifelse(varname_normalized %in% c('do_actor_a'), value_normalized, NA)) %>%
               mutate(do_actor_b= ifelse(varname_normalized %in% c('do_actor_b'), value_normalized, NA)) %>%
               mutate(say_actor_a= ifelse(varname_normalized %in% c('say_actor_a'), value_normalized, NA)) %>%
               mutate(say_actor_b= ifelse(varname_normalized %in% c('say_actor_b'), value_normalized, NA)) %>%
               mutate(think_actor_a= ifelse(varname_normalized %in% c('think_actor_a'), value_normalized, NA)) %>%
               group_by(crisno, sentence_number_int_aligned, sentence_span_text, event_number_int) %>%
               fill(do_actor_a,do_actor_b,say_actor_a,say_actor_b,think_actor_a, .direction ="updown") %>%
               group_by(crisno, sentence_number_int_aligned, sentence_span_text , do_actor_a, do_actor_b, say_actor_a, say_actor_b, think_actor_a, varname_normalized) %>%
               summarise(value_normalized = value_normalized %>% unique() %>% sort() %>% paste(collapse=";")) 
  
               
events_agreed_long_filtered_experts_atleast2_wide <-   temp %>%
  filter(!varname_normalized %in% c('crisno','sentence_number_int_aligned','sentence_span_text','think_actor_a','say_actor_a','say_actor_b','do_actor_a','do_actor_b',
                                    'crisis', 'icb_survey_version', 'section', 'sentencenumber')) %>%
  pivot_wider( id_cols=c(crisno, sentence_number_int_aligned, sentence_span_text , think_actor_a, say_actor_a, say_actor_b, do_actor_a, do_actor_b  ) ,
               names_from=varname_normalized,
               values_from=value_normalized)

dim(events_agreed_long_filtered_experts_atleast2_wide) #21660    74

temp <- events_agreed_long_filtered_experts_atleast2_wide %>% mutate_all(as.character) %>% mutate_all(trimws) %>% as.matrix()
temp[temp==""]=NA
events_agreed_long_filtered_experts_atleast2_wide$count_filled_fields <-  rowSums(!is.na(temp))

events_agreed_long_filtered_experts_atleast2_wide_topevent <- events_agreed_long_filtered_experts_atleast2_wide %>%
  arrange(crisno, sentence_number_int_aligned, sentence_span_text , desc(count_filled_fields)) %>%
  group_by(crisno, sentence_number_int_aligned, sentence_span_text ) %>%
  filter(row_number()==1)

#events_agreed_long_filtered_experts_atleast2 %>% count(varname_normalized) %>%
#  filter(varname_normalized %>% str_detect('actor'))

#This already collapses codings to specific events.
#Every unique set of actors got an event. 
#events_agreed_wide <- readRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/ICBe_V1.1_events_agreed.Rds")
#events_agreed_wide_filtered <- events_agreed_wide %>% filter(sentence_span_text %>% str_detect("\\.$")) #Removes 626 degenerate sentence 

#To minimize code rewrites we just rename it to the older filtered name, but know that it's the highly filtered, 2 experts, 1 event per, with the most agreed fields.
events_agreed_wide_filtered <- events_agreed_long_filtered_experts_atleast2_wide_topevent

#Ok new plan, we're going to keep at most one of each type, so one thought, one speech, one action, and a sentence can have each.

#We're going to also only take 1 event per sentence. The one with the most details tends to be the one with the most agreement.
temp <- events_agreed_wide_filtered %>% mutate_all(as.character) %>% mutate_all(trimws) %>% as.matrix()
temp[temp==""]=NA
events_agreed_wide_filtered$count_filled_fields <-  rowSums(!is.na(temp))
events_agreed_wide_filtered <- events_agreed_wide_filtered %>%
                               mutate_if(is.character, list(~na_if(.,""))) %>% #make sure you replace empty quots with NAs or it'll break everything
                               mutate(event_type_thought = !is.na(think_actor_a) ) %>%
                               mutate(event_type_say = !is.na(say_actor_a) | !is.na(say_actor_b) ) %>%
                               mutate(event_type_do = !is.na(do_actor_a) | !is.na(do_actor_b) ) %>% #if either actor isn't missing it's a do
                               #mutate(event_type_do_act = !is.na(do_actor_a) & is.na(do_actor_b) ) %>% #we're punting on this now plus it was bugged
                               #mutate(event_type_do_interact = !is.na(do_actor_a) & !is.na(do_actor_b) ) %>%
                               mutate(event_type_yesno = ifelse(is.na(event_type), "does not have an event","has an event" )) %>% 
  
                               mutate(event_type_armed_vs_unarmed = case_when( 
                                        !is.na(interact_decreasecoop) | !is.na(interact_increasecoop) | !is.na(act_uncooperative) | !is.na(act_cooperative)   ~ "unarmed",
                                        !is.na(interact_escalate) | !is.na(interact_deescalate) | !is.na(act_escalate) | !is.na(act_deescalate)   ~ "armed",
                                 )
                                ) %>%
                               arrange(count_filled_fields %>% desc()) %>% 
                               #group_by(crisno, sentence_number_int_aligned,event_type_new) %>%  #keeping one of each type
                               # filter(row_number()==1) %>% 
                               group_by(crisno, sentence_number_int_aligned) %>%  #Now just keeping only one per, whichever had the most details
                                 filter(row_number()==1) %>% 
                               ungroup() %>%
                               group_by(crisno, sentence_number_int_aligned) %>%
                                   mutate(
                                     sentence_type_includes_thought = max(event_type_thought),
                                     sentence_type_includes_say = max(event_type_say),
                                     sentence_type_includes_do = max(event_type_do)
                                   ) %>%
                               ungroup() %>% 
                                mutate(do_leaf=coalesce(interact_increasecoop, interact_decreasecoop, interact_deescalate, interact_escalate,
                                      act_escalate, act_deescalate, act_cooperative, act_uncooperative)) #%>% #this fails because '' are being treated as values
                               #not doing because it should be very sparse now
                               #mutate_all(replace_na, 'none'  ) #have to do this last because the codings above depend on NAs to exist

#Some cleaning
#Need to recode event type based on what actors are listed
events_agreed_wide_filtered <- events_agreed_wide_filtered 


#Take 80% for training
#10% for validation
#10% for test

icbe_crises    <-  events_agreed_wide_filtered %>% count(crisno) 
nrow(icbe_crises) #475
train_icbe_crises <- icbe_crises %>% arrange(crisno) %>% filter(row_number()<=375) %>% pull(crisno)
#[1] 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425
valid_icbe_crises <- icbe_crises %>% arrange(crisno) %>% filter(row_number()>375 & row_number()<=425 ) %>% pull(crisno)
test_icbe_crises <- icbe_crises %>% arrange(crisno) %>% filter(row_number()>425) %>% pull(crisno)
#verify no overlap

set.seed(1)
train_events_agreed_long_filtered_experts_atleast2 <- events_agreed_long_filtered_experts_atleast2 %>% filter(crisno %in% train_icbe_crises)

train_events_agreed_wide_filtered <- events_agreed_wide_filtered %>% arrange(crisno, sentence_number_int_aligned) %>% filter(crisno %in% train_icbe_crises) #8475    71
valid_events_agreed_wide_filtered <- events_agreed_wide_filtered %>% arrange(crisno, sentence_number_int_aligned) %>% filter(crisno %in% valid_icbe_crises) #%>%
                                        #group_by(event_type) %>%
                                        #slice_sample(n=50) %>%
                                        #filter(row_number()<50) %>% 
                                        #ungroup() #1598   71 #For testing purposes limiting to the first 100
test_events_agreed_wide_filtered <- events_agreed_wide_filtered %>% arrange(crisno, sentence_number_int_aligned) %>% filter(crisno %in% test_icbe_crises)  # %>% filter(row_number()<2) #2058   71

#library(textreuse) #install.packages('textreuse')
#minhash <- minhash_generator(n = 240, seed = 3552)
#head(minhash(c("turn tokens into", "tokens into hashes", "into hashes fast")))

events_agreed_wide_filtered %>% write_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/events_agreed_wide_filtered.csv")
events_agreed_wide_filtered %>% dplyr::select(sentence_span_text, starts_with("do_"), starts_with("interact_"), starts_with("act_"))

train_events_agreed_wide_filtered %>% saveRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/train_events_agreed_wide_filtered.Rds")
valid_events_agreed_wide_filtered %>% saveRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/valid_events_agreed_wide_filtered.Rds")
test_events_agreed_wide_filtered  %>% saveRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/test_events_agreed_wide_filtered.Rds")

table( train_events_agreed_wide_filtered$event_type_armed_vs_unarmed,  useNA = "always")
table(train_events_agreed_wide_filtered$do_leaf, train_events_agreed_wide_filtered$event_type_armed_vs_unarmed,  useNA = "always")

```

# Working Functions

## Function: Assemble Existing Answers

This dumps two datasets of wide predictions to global
valid_predictions_wider
test_predictions_wider

```{r}

coalesce_vars <- c('interact_increasecoop','interact_decreasecoop','interact_deescalate','interact_escalate','act_escalate','act_deescalate','act_cooperative','act_uncooperative')

regenerate_predictions_wide <- function(){
    
  valid_blank <- valid_events_agreed_wide_filtered %>% 
                 dplyr::select(crisno,  sentence_number_int_aligned, sentence_span_text ) %>% #crisis_text
                 arrange(crisno, sentence_number_int_aligned)
  
  path_validation_predictions <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/predictions_long_validation/"
  files_validation_predictions <- path_validation_predictions %>% list.files(pattern = NULL, all.files = FALSE,full.names = TRUE)
  valid_predictions_long <- files_validation_predictions %>% lapply(function(x) read_csv(x,col_types=rep('c',100) %>% paste0(collapse='') )) %>% bind_rows() 
  
  if(nrow(valid_predictions_long)==0){
    valid_predictions_wider <<- valid_blank
  } else {
    valid_predictions_wider <<- valid_blank %>% left_join( valid_predictions_long %>% dplyr::select(-starts_with('prompt'))  %>%
                                pivot_wider(
                                  names_from = variable,
                                  values_from = value#,
                                  #values_fn = paste0(sep=";", collapse=";")
                                ) %>% rename(sentence_span_text=sentence)  ) %>% mutate_all(as.character) %>% mutate_all(trimws) %>% distinct()
    if(length(setdiff(coalesce_vars, names(valid_predictions_wider)))==0  ){
        valid_predictions_wider <<- valid_predictions_wider %>% 
                                    mutate(do_leaf=coalesce(
                                      #!!! rlang::syms(uCols))
                                          interact_increasecoop, interact_decreasecoop, interact_deescalate, interact_escalate,
                                          act_escalate, act_deescalate, act_cooperative, act_uncooperative
                                    ))  
      }
  }
  test_blank  <- test_events_agreed_wide_filtered %>% 
                 dplyr::select(crisno,  sentence_number_int_aligned, sentence_span_text ) %>% #crisis_text
                 arrange(crisno, sentence_number_int_aligned)
  
  path_test_predictions <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/predictions_long_test/"
  files_test_predictions <- path_test_predictions %>% list.files(pattern = NULL, all.files = FALSE,full.names = TRUE)
  test_predictions_long <- files_test_predictions %>% lapply(function(x) read_csv(x,col_types=rep('c',100) %>% paste0(collapse='') )) %>% bind_rows() 

  if(nrow(test_predictions_long)==0){
    test_predictions_wider <<- test_blank
  } else {
    test_predictions_wider <<- test_blank %>% left_join( test_predictions_long %>% dplyr::select(-starts_with('prompt')) %>% 
                              pivot_wider(
                                names_from = variable,
                                values_from = value#,
                                #values_fn = paste0(sep=";", collapse=";")
                              ) %>% rename(sentence_span_text=sentence)  ) %>% mutate_all(as.character) %>% mutate_all(trimws) %>% distinct()
  
    if(length(setdiff(coalesce_vars, names(test_predictions_wider)))==0  ){
      test_predictions_wider <<- test_predictions_wider %>%  mutate(do_leaf=coalesce(interact_increasecoop, interact_decreasecoop, interact_deescalate, interact_escalate,
                                  act_escalate, act_deescalate, act_cooperative, act_uncooperative))  
    }
  }

}

```


## Function: Code a Question

Takes in a vector of variables and sentences to code and returns a data frame with the predicted values

```{r}

crisis_narratives <-  readRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.Rds")

main_path = "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/predictions_long_validation/"
#main_path = "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/predictions_long_test/"

library(stringdist) #install.packages('stringdist')
#stringdist(a, b, method='lv')

#install.packages("progress")
library(progress) #https://github.com/r-lib/progress
#variables=c('event_type_yesno')
#sentences <- valid_events_agreed_wide_filtered$sentence_span_text[1:10]
sentences=NULL #some package is loading an object
classify_many_sentences <- function(
                                    variables, 
                                    #sentences, #we don't pass this in anymore
                                    verbose=FALSE,
                                    group_size_max=40,  #Pick the largest you can get away with without an OOM
                                    fileout=NULL,
                                    stratify_variables=NULL,
                                    keep_variables=NULL,
                                    training_constraints ="TRUE",
                                    predicting_constraints="TRUE",
                                    preamble=NULL,
                                    include_crisis=T
                                    ){
  if(is.null(stratify_variables)){stratify_variables=variables}
  if(is.null(keep_variables)){keep_variables=variables}
  if(is.null(fileout)){
    fileout = paste0(main_path,keep_variables,".csv")
  } 
  if(fileout %>% file.exists()){
    return(fileout %>% read_csv( na = c("") )) #We now have a notion of NA meaning observed missing so only load quotes as true na
  } 
  
  #Grab sentences to code
  regenerate_predictions_wide() #regenerate the data we have
  condition_vars=setdiff(variables,keep_variables)
  #I think we only change this sentence and the directory path at the end to switch it to test
  sentences_df <- valid_predictions_wider %>% filter( !! rlang::parse_expr(predicting_constraints) ) %>% dplyr::select(crisno,sentence_span_text, one_of(condition_vars))
  #sentences_df <- test_predictions_wider %>% filter( !! rlang::parse_expr(predicting_constraints) ) %>% dplyr::select(crisno,sentence_span_text, one_of(condition_vars))

  #Subset training data
  train_xy <- train_events_agreed_wide_filtered  %>% 
              filter( !! rlang::parse_expr(training_constraints) ) %>%  #https://www.r-bloggers.com/2020/09/using-dplyrfilter-when-the-condition-is-a-string/
              dplyr::select(crisno, sentence_number_int_aligned, sentence_span_text, one_of(variables, stratify_variables)) %>%
              unite("everything",one_of(c('sentence_span_text',variables)), sep="|", remove = FALSE, na.rm = FALSE) %>% 
              unite("strata", one_of(stratify_variables), sep="|", remove = FALSE, na.rm = FALSE) %>%
              mutate(strata_lumped = strata %>% fct_lump_n(20) )  #top 10 categories plus an other
  #valid_xy <- valid_events_agreed_wide_filtered %>% dplyr::select(crisno, sentence_number_int_aligned, sentence_span_text, one_of(variables))
  #test_xy <- test_events_agreed_wide_filtered %>% dplyr::select(crisno, sentence_number_int_aligned, sentence_span_text, one_of(variables))
  
  train_xy <- train_xy %>%
              rowwise() %>%
                mutate(n_tokens = py$generator$tokenizer$encode(everything)$shape[1]) %>%
              ungroup()
  
  condition <- (!(train_xy[,variables] %>% as.matrix() %>% is.na())) %>% rowSums() %>% as.integer()
  
  #If we end up with too many categories, take only the most common and then recode the minor as "other"
  category_counts <- train_xy %>%
                     count(strata_lumped) 
    
  n_categories <- train_xy %>% 
                  filter(condition>0) %>%
                  #select(one_of(stratify_variables)) %>%
                  select(strata_lumped) %>%
                  distinct() %>%
                  nrow()
  
  pb <- progress_bar$new(format = "predicting [:bar] :percent eta: :eta", total = nrow(sentences_df), clear = FALSE, width= 60)
  pb$tick(0)
  answer_df_list = list()
  for(i in 1:nrow(sentences_df)  ){
    pb$tick()
    crisisno=sentences_df$crisno[i]
    crisis_text = crisis_narratives %>% filter(crisno==crisisno) %>% pull(text) %>% str_replace_all("[\n\r]{1,}",'\n') %>% str_replace_all("\n {1,}\n",'\n') %>% str_split("References")
    crisis_text = crisis_text[[1]][1] %>% trimws() 
    #Take the first 500 tokens
    crisis_text_tokens <- py$generator$tokenizer$encode(crisis_text)
    n_crisis_text_tokens=crisis_text_tokens$shape[1]
    crisis_token_budget=min(n_crisis_text_tokens,500)-1
    crisis_text_short <- py$generator$tokenizer$decode( crisis_text_tokens[0][0:crisis_token_budget] )
    sentence=sentences_df$sentence_span_text[i]
    out_sentence = sentences_df %>% filter(row_number()==i) %>% dplyr::select(sentence_span_text,one_of(condition_vars)) %>% unite('out',sep="|") %>% pull(out)
    n_token_sentence = py$generator$tokenizer$encode(sentence)$shape[1]
    n_token_preamble = py$generator$tokenizer$encode(preamble)$shape[1]
    token_budget = max_tokens - 75 - n_token_preamble - n_token_sentence - (crisis_token_budget*include_crisis) #adding a buffer
    #Instead of groupsize, you get a token budget. That's something like 2700 - the prompt - 3 more per row
    group_token_budget <- floor(token_budget/n_categories)
    
    #Different draws lead to different answers. That's not great.
    train_sample <- train_xy %>% 
                    filter(condition>0) %>%
                    filter(sentence_span_text %>% nchar() < 190 ) %>% #reject very long example sentences
                    mutate(sdists= sentence_span_text %>% tolower() %>% stringdist( sentence %>% tolower(), method='osa') ) %>% #cosine #lv
                    mutate(random=runif(n())) %>%
                    arrange(sdists ) %>% #sort by training strings with the smallest distance to the target string
                    filter(!duplicated(sentence_span_text)) %>% #don't include multiple events from the same sentence in the training sample
                    #When there are too many strata we can end up with less than 1 example for every group.
                    #group_by(pick({{ stratify_variables }})) %>% #https://dplyr.tidyverse.org/reference/pick.html
                    group_by( strata_lumped ) %>% #https://dplyr.tidyverse.org/reference/pick.html
                        mutate(group_tokens=cumsum(n_tokens+4)) %>% #this assumption of 4 doesn't hold for long answers (it should now, added answers intot the token count)
                        #filter(row_number()<=group_size) %>% #upping the n key
                    ungroup() %>%
                    group_by( strata_lumped ) %>% #https://dplyr.tidyverse.org/reference/pick.html
                      filter(group_tokens<group_token_budget) %>% #upping the n key
                    ungroup() %>%
                    mutate(random=runif(n())) %>% #I don't know why but somehow it looks less random unless I regenerate this number
                    arrange(random) %>%
                    dplyr::select(sentence_span_text, one_of(variables)) %>% 
                    mutate_all(trimws  ) %>% #treating NA as observed none
                    mutate_all(list(~na_if(.,""))) %>%
                    mutate_all(replace_na, 'none'  ) %>% 
                    dplyr::select(sentence=sentence_span_text, one_of(variables)) %>%
                    #one last token count and threshold
                    unite("out", everything(), remove = FALSE, sep="|") %>%  
                    rowwise() %>%
                      mutate(n_tokens = py$generator$tokenizer$encode(out)$shape[1]) %>%
                    ungroup() %>%
                    mutate(n_tokens_cumsum=cumsum(n_tokens+2)) %>% #add one for the newline character
                    filter(n_tokens_cumsum<token_budget)

    dim(train_sample) #
    
    #train_sample_md <- train_sample %>% dplyr::select(sentence=sentence_span_text, one_of(variables)) %>% knitr::kable('pipe') %>% 
    #  str_replace_all('-{1,}','-') %>% str_replace_all(' {1,}',' ') %>% paste(collapse="\n") # %>% writeLines()
    #savess about 141 tokens
    train_sample_md <- train_sample %>% pull(out) %>% paste(collapse="\n")
    #nchar(train_sample_md)/4 #You can end up with a sample that's too long because of many too long sentences
    if(include_crisis){
      prompt= glue::glue({preamble},"\nBegin Examples\n", {train_sample_md},"\nEnd Examples\n\nBegin Crisis Narrative\n", {crisis_text_short}, "...\nEnd Crisis Narrative\n\nBegin Crisis Sentence to Code\n",  {out_sentence},"|")
    } else {
      prompt= glue::glue({preamble},"\nBegin Examples\n", {train_sample_md},"\n",  {out_sentence},"|")      
    }
    n_tokens=py$generator$tokenizer$encode(prompt)$shape[1]
    #print(n_tokens)
    if(n_tokens>max_tokens){
      print(sentence)
      stop("too many tokens")
    }
    output =  py$generator$generate_simple_rex( prompt , max_new_tokens = as.integer(100) , custom_stop= '\n'  ) #
    response= output %>% str_replace(prompt %>% as.character() %>% fixed(),"") #the extra variables are built into the prompt and removed
    response_clean <- (response %>% str_split(fixed("|")))[[1]][1:length(keep_variables)]  %>% trimws()
    names(response_clean) <- keep_variables
    temp_df= as.data.frame(response_clean %>% t())
    temp_df$sentence <- sentence
    temp_df$prompt <- prompt
    answer_df_list[[sentence]] <- temp_df
  }
  final_df_long <- bind_rows(answer_df_list) %>%
                   pivot_longer(-c(sentence,prompt), names_to = "variable", values_to = "value") %>%
                   filter(variable %in% keep_variables)
  
  final_df_long %>% write_csv(fileout)
  regenerate_predictions_wide() #one more time just so we're up to date
  return(final_df_long)
}


```




# step00_chunk Chunk the Story

Chunk

```{r}

crisis_narratives <-  readRDS("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_in/crises_narratives_rex_2023_webscrape.Rds")

for(i in 1:nrow(crisis_narratives)){
  crisno=crisis_narratives$crisno[i]
  fileout=glue("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step00_chunk/{crisno}.csv")
  print(fileout)
  if(  file.exists(fileout) ){
    next
  }
  chunks=strsplit(crisis_narratives$text[i], split="[\r\n]+")[[1]]
  df= data.frame(chunk=chunks) %>% mutate(crisno=crisno) %>% mutate(chunk=chunk %>% trimws()) %>% filter(nchar(chunk)>0) %>% mutate(chunk_number=row_number())
  df %>% write_csv(fileout)
}

```



# step01_chunk_classified Classify Each Chunk

```{r}

question = glue("Which of these best describes the text span?
A) A text fragment like a title, section heading, reference, or other incomplete sentence
B) 1 complete sentence
C) 2 or more complete sentences")
  
in_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step00_chunk/"
out_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step01_chunk_classified/"
  
in_files <- list.files(path = in_directory, full.names = T)
for(in_file in in_files){
  outfile <- glue(out_directory, basename(in_file))
  if(file.exists(outfile)){ next }
  print(outfile)
  in_df <- read_csv(in_file)
  
  df_list <- list()
  out_df=in_df
  out_df$chunk_type=NA
  for(i in 1:nrow(in_df)){
        chunk=in_df$chunk[i]
        #print(chunk)
        prompt <- glue(
        '### Instruction: Read the following text span and be prepared to answer a question about it.\n',
        'BEGIN TEXT SPAN\n{chunk}\nEND TEXT SPAN\n',
        'BEGIN QUESTION\n{question}\nEND QUESTION\n',
        "BEGIN ANSWER LETTER\n### Response:'"
        )
        #tic()
          output =  py$generator$generate_simple_rex( prompt , max_new_tokens = as.integer(3) , custom_stop= "'"  ) #
        #generation_time=toc()
        response= output %>% str_replace(prompt %>% as.character() %>% fixed(),"") %>% trimws() #the extra variables are built into the prompt and removed
        #print(response)
        out_df$chunk_type[i]<-response %>% str_replace("[^A-Ca-c]","") %>% recode(A = "fragment", B = "sentence", C="sentences")
    }
  out_df %>% write_csv(outfile, na = "")
}



```


# step02_sentence Sentence Split

```{r}

in_directory <- "//media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step01_chunk_classified/"
out_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step02_sentence/"
  
in_files <- list.files(path = in_directory, full.names = T)
for(in_file in in_files){
  outfile <- glue(out_directory, basename(in_file))
  if(file.exists(outfile)){ next }
  print(outfile)
  in_df <- read_csv(in_file)
  
  df_list <- list()
  out_df=in_df
  out_df$sentence_list=NA
  for(i in 1:nrow(in_df)){
        chunk=in_df$chunk[i]
        if(!in_df$chunk_type[i] %in% c('sentences')){next}
        #print(chunk)
        prompt <- glue(
          '### Instruction: Split the following text span into a numbered list of individual sentences.\n',
          'BEGIN TEXT SPAN\n{chunk}\nEND TEXT SPAN\n',
          'BEGIN SPLIT SENTENCES (print @ when done answering)\n### Response:'
        )
        #tic()
          output =  py$generator$generate_simple_rex( prompt , max_new_tokens = as.integer(1500) , custom_stop= '@'  ) #
        #generation_time=toc()
        response= output %>% str_replace(prompt %>% as.character() %>% fixed(),"") %>% trimws() #the extra variables are built into the prompt and removed
        #print(response)
        out_df$sentence_list[i]<-response %>% trimws()
  }
  out_df %>% write_csv(outfile, na = "")
}

#Combine
in_files <- list.files(path = out_directory, full.names = T)
df <- in_files %>% lapply(read_csv) %>% bind_rows()
df_sentences  <- df %>% 
    mutate(sentence= ifelse(is.na(sentence_list), chunk, sentence_list)) %>%
    mutate(sentence = strsplit(as.character(sentence), "\n")) %>% 
    unnest(sentence, keep_empty=T) %>%
    mutate(sentence = sentence %>% str_replace("^[0-9\\. ]*", "")  %>% trimws() ) %>%
    group_by(crisno) %>%
      mutate(sentence_number=row_number()) %>%
    ungroup() %>%
    group_by(crisno, chunk_type=='fragment') %>%
      mutate(sentence_number_renumbered=row_number()) %>%
    ungroup() %>%
    mutate(sentence_number_renumbered = ifelse(chunk_type=='fragment', NA, sentence_number_renumbered)) 

df_sentences %>% write_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/icbe_sentences_llm.csv", na = "")
  
```


# step03 Event Split

```{r}

icbe_sentences_llm <- read_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/icbe_sentences_llm.csv")

in_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step02_sentence/"
out_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step03_events/"

for(crisnumber in (icbe_sentences_llm$crisno %>% unique() %>% sort())   ){

  #crisnumber=377 #376
  #crisnumber=196 #376
  outfile <- paste0(out_directory,"/",crisnumber,".csv")
  if(file.exists(outfile)){
    next
  }
  print(outfile)
  df_in <- icbe_sentences_llm %>% filter(crisno==crisnumber)
  df_in$events <- ""
  for(i in 1:nrow(df_in)){
    if(df_in$chunk_type[i]=="fragment"){ next }

      #sentence="That day Cuba responded by condemning the U.S. blockade and declaring its willingness to fight."
      sentence=df_in$sentence[i]
      chunk = df_in$chunk[i]
      
      print("")
      print(sentence)
      prompt0 <- glue('You are a military historian building a conflict event dataset.\n\n',
                     '### Instruction: Carefully read this sentence and be prepared to list every distinct event it describes.\n\n',
                     'Paragraph: "{chunk}"\n\n',
                     'Specific Sentence: "{sentence}"\n\n',
                    "### Response: The task is event dissagregation. Sometimes a sentence might describe several different things that happened. I have rewritten that sentence into several complete sentences that each describe a different event that took place. I understand each disaggregated sentence must stand alone and contain all the nescessary information to be coded by an NLP pipeline without access to the full text. I therefore paid careful attention to any mention of an actor doing something, communicating something, or thinking/believing something. In each case, I was clear about who performed the action and if applicable who the action was directed toward or who was the audience. I have printed the new complete sentences as a python list (['','',...]). The sentence '{sentence}' is best dissagregated into different unique events as the following complete sentences ['"
                    )
      #writeLines(prompt0)
      output0 =  py$generator$generate_simple_rex( prompt0 , max_new_tokens = as.integer(300) , custom_stop= ']'  ) #
      response0= output0 %>% str_replace(prompt0 %>% as.character() %>% fixed(),"") %>% trimws() #the extra variables are built into the prompt and removed
      writeLines(response0)
      df_in$events[i] <- response0
  }
  df_in %>% write_csv(outfile)
}



#Combine
in_files <- list.files(path = "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step03_events/", full.names = T)
df <- in_files %>% lapply(read_csv) %>% bind_rows()
df_events  <- df %>% 
    #mutate(sentence= ifelse(is.na(sentence_list), chunk, sentence_list)) %>%
    mutate(event = strsplit(as.character(events), "',")) %>% 
    unnest(event, keep_empty=T) %>%
    mutate(event = event %>% str_replace_all("[\'\"]", "")  %>% trimws() ) %>%
    group_by(crisno,sentence_number) %>%
      mutate(event_number=row_number()) %>%
    ungroup() %>%
    group_by(crisno, chunk_type=='fragment') %>%
      mutate(sentence_number_renumbered=row_number()) %>%
    ungroup() %>%
    dplyr::select(-starts_with("chunk_type ==" )) %>%
    mutate(sentence_number_renumbered = ifelse(chunk_type=='fragment', NA, sentence_number_renumbered)) 

df_events %>% write_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/icbe_events_llm.csv", na = "")


```




# step03 Rewrite

```{r, eval=F}

df_events <- read_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/icbe_events_llm.csv", na = "")

in_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step03_events/"
out_directory <- "/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/step03_events_rewrite/"

for(crisnumber in (df_events$crisno %>% unique() %>% sort())   ){

  #crisnumber=377 #376
  #crisnumber=196 #376
  outfile <- paste0(out_directory,"/",crisnumber,".csv")
  if(file.exists(outfile)){
    next
  }
  print(outfile)
  df_in <- df_events %>% filter(crisno==crisnumber)
  df_in$events_rewrite <- ""
  df_in$events_rewrite_compound <- ""
  for(i in 1:nrow(df_in)){
    if(df_in$chunk_type[i]=="fragment"){ next }

      #sentence="That day Cuba responded by condemning the U.S. blockade and declaring its willingness to fight."
      event=df_in$event[i]
      sentence=df_in$sentence[i]
      chunk = df_in$chunk[i]
      
      print("")
      print(sentence)
      print(event)
      prompt0 <- glue('You are a military historian building a conflict event dataset.\n\n',
                     '### Instruction: Carefully read this paragraph, sentence, and draft event and be prepared to rewrite the event to be more detailed and standalone.\n\n',
                     'Paragraph: "{chunk}"\n\n',
                     'Specific Sentence: "{sentence}"\n\n',
                     'Draft Event: "{event}"\n\n',
                    "### Response: Sometimes a sentence will cram in multiple distinct events. An earlier pipeline dissagregated the sentence into seperate specific events. The draft event is the one that I am currently working on. My task is to make it easier to process by the event NLP pipeline. The first step classifies the event as a military action, civilian action, communication, or thought by one of the actors. The second step determines the actor who initiated that action. The third step determines if there was a target / audience for that action or communication. A fourth step extracts information about location and timing of the event. To make those NLP steps easier to perform, the draft event '{event}' is best rewritten as '"
                    )  
      #writeLines(prompt0)
      output0 =  py$generator$generate_simple_rex( prompt0 , max_new_tokens = as.integer(300) , custom_stop= "'"  ) #
      response0= output0 %>% str_replace(prompt0 %>% as.character() %>% fixed(),"") %>% trimws() #the extra variables are built into the prompt and removed
      writeLines(response0)
      df_in$events_rewrite[i] <- response0
      events_rewrite <- df_in$events_rewrite[i] 
        
      prompt0 <- glue('You are a military historian building a conflict event dataset.\n\n',
                     "### Instruction: Your task is sentence decomposition for diplomatic and military events. Given a sentence about a communication, perception, or action by one country towards another (or self-action), decompose it into simpler sentences that capture the primary information and the subsequent details.
Steps:
Identify the Core Action or State: Begin by determining the principal action or state described in the given sentence. This could be a communication, a perception, or a standalone action.
Split Communication from Action: If the sentence describes a communication about an action or state, create two distinct sentences: a. One detailing the act of communication. b. The other detailing the content of that communication. Similarly, for perceptions or thoughts about actions, follow the same split. 
Retain Standalone Actions: If the sentence primarily describes a standalone action without being nested within a communication or perception, retain it as-is. Ensure Clarity: The decomposed sentences should be clear and stand alone in meaning without depending on the other sentences. Remove or rewrite any redundancies while maintaining the core information.
Example: Given: 'Country A formally demanded that Country B halt military drills close to their shared frontier.' Decompose into: ['Country A made a formal demand to Country B.', 'Country B cease military drills near the shared frontier.'] 
Multiple Actions: If there are multiple distinct actions or states nested within a communication or perception, break each one into its own sentence. Make sure the order of the decomposed sentences makes logical sense.
Use Neutral Tone: The tone should remain neutral, factual, and in-line with a style found in diplomatic dispatches or history books. Maintain consistency in the naming throughout the task.
Maintain Syntax and Grammar: Ensure that each decomposed sentence is grammatically correct and retains proper syntax.
Completion: Once all steps are followed, review the decomposed sentences to ensure they collectively represent the information from the original sentence without distortion. Your task is to apply these steps to the sentences provided and decompose them into simpler sentences. Remember to maintain clarity, neutrality, and consistency throughout.\n\n",
"Tutorial Examples:
'Country A communicated a demand to Country B to end the military exercises near the border.' = ['Country A communicated a demand to Country B.', 'Country B end the military exercises near the border.']
'Country A perceived a start of a crisis when Country B mobilized its military near the disputed area.' = ['Country A perceived a start of a crisis.', 'Country B mobilized its military near the disputed area.']
'Country A communicated an appeal to Country B to consider humanitarian aid for the war-torn region.' = ['Country A communicated an appeal to Country B.', 'Country B consider humanitarian aid for the war-torn region.']
'Country A conducted a military exercise.' = ['Country A conducted a military exercise.']
'Country A communicated an acceptance of Country B's terms for signing a mutual defense pact.' = ['Country A communicated an acceptance of Country B's terms.', 'Country B's terms for signing a mutual defense pact.']
'Country A, holding a fear of escalation, called for a meeting with Country B.' = ['Country A held a fear of escalation.', 'Country A called for a meeting with Country B.']
'Country A communicated an accusation against Country B for violating the no-fly zone.' = ['Country A communicated an accusation against Country B.', 'Country B violating the no-fly zone.']
'Country A communicated a threat to impose sanctions if Country B continues its nuclear weapons test.' = ['Country A communicated a threat to Country B.', 'Country B continues its nuclear weapons test.']
'Country A, experiencing discovering a fact about Country B's hidden arsenal, initiated intelligence cooperation with Country C.'
= ['Country A experienced discovering a fact about Country B's hidden arsenal.', 'Country A initiated intelligence cooperation with Country C.']
'Country A communicated praise to Country B for allowing access to humanitarian organizations in conflict areas.'
= ['Country A communicated praise to Country B.', 'Country B allowing access to humanitarian organizations in conflict areas.']
'Country A, perceiving an end of a crisis, began withdrawing diplomats from Country B.'
= ['Country A perceived an end of a crisis.', 'Country A began withdrawing diplomats from Country B.']
'Country A communicated a rejection of Country B's proposal for economic cooperation amidst the global downturn.'
= ['Country A communicated a rejection of Country B's proposal.', 'Country B's proposal for economic cooperation.']\n\n",
"Begin Real Text:\n",
                     'Paragraph: "{chunk}"\n\n',
                     #'Specific Sentence: "{sentence}"\n\n',
                     'Event: "{events_rewrite}"\n\n',
                    "### Response: Carefully following the instructions, and practice examples above, the real text and the event '{event}' is best decomposed as ['"
                    )  
      
      
      prompt0 <- glue('You are a military historian building a conflict event dataset.\n\n',
                     "### Instruction: You are tasked with simplifying sentences about diplomatic and military events. Given a sentence about a country's action, communication, or perception, break it down into simpler, clear, and neutral-toned sentences.

Steps:
Identify the Core Action or State: Recognize the main action or state in the sentence. This can be a communication, perception, or action.
Separate Communication from Action:
If the sentence is about a communication regarding an action or state, create two distinct sentences:
a. One for the act of communication.
b. One for the content of the communication.
For perceptions or thoughts about actions, use the same split.
Retain Standalone Actions: If the sentence is primarily an action, keep it unchanged.
Multiple Actions: If there's more than one action or state in a communication or perception, separate each into its own sentence. Ensure the sentences are logically ordered.
Maintain Neutrality and Clarity: Ensure a neutral tone suitable for history books or diplomatic records. Each simplified sentence should be clear without relying on other sentences.
Check Grammar: Each simplified sentence should be grammatically correct.
Review: Once simplified, review the sentences to ensure they accurately represent the original information.

Examples:
Given: 'Country A invaded Country B.'
Simplified: ['Country A invaded Country B.']
Given: 'Country A formally demanded that Country B halt military drills close to their shared frontier.'
Simplified: ['Country A made a formal demand to Country B.', 'Country B cease military drills near the shared frontier.']
Given: 'Country A perceived a crisis start when Country B mobilized its military near a disputed area.'
Simplified: ['Country A perceived a crisis start.', 'Country B mobilized its military near a disputed area.']

Task:
Background paragraph: '{chunk}'
Given: '{events_rewrite}'

Your task: Simplify the event following the steps and examples provided.\n",

"### Response: Carefully following the instructions, and practice examples above, the event '{event}' is best decomposed as ['"
                    )  
      #writeLines(prompt0)
      output0 =  py$generator$generate_simple_rex( prompt0 , max_new_tokens = as.integer(300) , custom_stop= "]"  ) #
      response0= output0 %>% str_replace(prompt0 %>% as.character() %>% fixed(),"") %>% trimws() #the extra variables are built into the prompt and removed
      writeLines(response0)
      df_in$events_rewrite_compound[i] <- response0      
      
  }
  df_in %>% write_csv(outfile)
}



#Combine
in_files <- list.files(path = out_directory, full.names = T)
df <- in_files %>% lapply(read_csv) %>% bind_rows()
df_events  <- df %>% 
    #mutate(sentence= ifelse(is.na(sentence_list), chunk, sentence_list)) %>%
    mutate(event = strsplit(as.character(events), "',")) %>% 
    unnest(event, keep_empty=T) %>%
    mutate(event = event %>% str_replace_all("[\'\"]", "")  %>% trimws() ) %>%
    group_by(crisno,sentence_number) %>%
      mutate(event_number=row_number()) %>%
    ungroup() %>%
    group_by(crisno, chunk_type=='fragment') %>%
      mutate(sentence_number_renumbered=row_number()) %>%
    ungroup() %>%
    mutate(sentence_number_renumbered = ifelse(chunk_type=='fragment', NA, sentence_number_renumbered)) 

df_events %>% write_csv("/media/skynet3/8tb_a/rwd_github_private/ICBeLLM/data_out/icbe_events_llm.csv", na = "")


```